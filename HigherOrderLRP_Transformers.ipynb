{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7baae42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.12) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "/home/farnoush/.local/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/home/farnoush/.local/lib/python3.8/site-packages/pandas/core/arrays/masked.py:64: UserWarning: Pandas requires version '1.3.2' or newer of 'bottleneck' (version '1.2.1' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import copy\n",
    "from torch.nn.modules import Module\n",
    "from torch import Tensor\n",
    "from torch import nn as nn\n",
    "import math\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, AutoTokenizer, AutoModelForPreTraining\n",
    "from IPython.display import display, HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from scipy import stats as st\n",
    "from datasets import load_dataset\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc348fd3",
   "metadata": {},
   "source": [
    "## Visualization Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "baa8b7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale_score_by_abs (score, max_score, min_score):\n",
    "    \"\"\"\n",
    "    Normalize the relevance value (=score), accordingly to the extremal relevance values (max_score and min_score), \n",
    "    for visualization with a diverging colormap.\n",
    "    i.e. rescale positive relevance to the range [0.5, 1.0], and negative relevance to the range [0.0, 0.5],\n",
    "    using the highest absolute relevance for linear interpolation.\n",
    "    \"\"\"\n",
    "    \n",
    "    # CASE 1: positive AND negative scores occur --------------------\n",
    "    if max_score>0 and min_score<0:\n",
    "    \n",
    "        if max_score >= abs(min_score):   # deepest color is positive\n",
    "            if score>=0:\n",
    "                return 0.5 + 0.5*(score/max_score)\n",
    "            else:\n",
    "                return 0.5 - 0.5*(abs(score)/max_score)\n",
    "\n",
    "        else:                             # deepest color is negative\n",
    "            if score>=0:\n",
    "                return 0.5 + 0.5*(score/abs(min_score))\n",
    "            else:\n",
    "                return 0.5 - 0.5*(score/min_score)   \n",
    "    \n",
    "    # CASE 2: ONLY positive scores occur -----------------------------       \n",
    "    elif max_score>0 and min_score>=0: \n",
    "        if max_score == min_score:\n",
    "            return 1.0\n",
    "        else:\n",
    "            return 0.5 + 0.5*(score/max_score)\n",
    "    \n",
    "    # CASE 3: ONLY negative scores occur -----------------------------\n",
    "    elif max_score<=0 and min_score<0: \n",
    "        if max_score == min_score:\n",
    "            return 0.0\n",
    "        else:\n",
    "            return 0.5 - 0.5*(score/min_score)    \n",
    "  \n",
    "      \n",
    "def getRGB (c_tuple):\n",
    "    return \"#%02x%02x%02x\"%(int(c_tuple[0]*255), int(c_tuple[1]*255), int(c_tuple[2]*255))\n",
    "\n",
    "     \n",
    "def span_word (word, score, colormap):\n",
    "    return \"<span style=\\\"background-color:\"+getRGB(colormap(score))+\"\\\">\"+word+\"</span>\"\n",
    "\n",
    "\n",
    "def html_heatmap (words, scores, cmap_name=\"bwr\"):\n",
    "    \"\"\"\n",
    "    Return word-level heatmap in HTML format,\n",
    "    with words being the list of words (as string),\n",
    "    scores the corresponding list of word-level relevance values,\n",
    "    and cmap_name the name of the matplotlib diverging colormap.\n",
    "    \"\"\"\n",
    "    \n",
    "    colormap  = plt.get_cmap(cmap_name)\n",
    "     \n",
    "    assert len(words)==len(scores)\n",
    "    max_s     = max(scores)\n",
    "    min_s     = min(scores)\n",
    "    \n",
    "    output_text = \"\"\n",
    "    \n",
    "    for idx, w in enumerate(words):\n",
    "        score       = rescale_score_by_abs(scores[idx], max_s, min_s)\n",
    "        output_text = output_text + span_word(w, score, colormap) + \" \"\n",
    "    \n",
    "    return output_text + \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9108a894",
   "metadata": {},
   "source": [
    "## LRP Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcc75667",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stabilize(z):\n",
    "    return z + ((z == 0.).to(z) + z.sign()) * 1e-6\n",
    "\n",
    "\n",
    "def modified_layer(\n",
    "        layer,\n",
    "        transform\n",
    "):\n",
    "    \"\"\"\n",
    "    This function creates a copy of a layer and modify\n",
    "    its parameters based on a transformation function 'transform'.\n",
    "    -------------------\n",
    "    :param layer: A layer which its parameters are going to be transformed.\n",
    "    :param transform: A transformation function.\n",
    "    :return: A new layer with modified parameters.\n",
    "    \"\"\"\n",
    "    new_layer = copy.deepcopy(layer)\n",
    "\n",
    "    try:\n",
    "        new_layer.weight = torch.nn.Parameter(transform(layer.weight.float()))\n",
    "    except AttributeError as e:\n",
    "        print(e)\n",
    "\n",
    "    try:\n",
    "        new_layer.bias = torch.nn.Parameter(transform(layer.bias.float()))\n",
    "    except AttributeError as e:\n",
    "        print(e)\n",
    "\n",
    "    return new_layer\n",
    "\n",
    "\n",
    "class ModifiedLinear(Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            fc,\n",
    "            transform\n",
    "    ):\n",
    "        super(ModifiedLinear, self).__init__()\n",
    "        self.fc = fc\n",
    "        \n",
    "        # TODO: Do not set bias to 0.\n",
    "        # self.fc.bias = torch.nn.Parameter(torch.zeros(self.fc.bias.shape))\n",
    "        \n",
    "        self.transform = transform\n",
    "        self.modified_fc = modified_layer(layer=fc, transform=transform)\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            x: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        z = self.fc(x)\n",
    "        zp = self.modified_fc(x)\n",
    "        zp = stabilize(zp)\n",
    "        return (zp.double() * (z.double() / zp.double()).data.double()).float()\n",
    "\n",
    "    \n",
    "class ModifiedLayerNorm(Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            norm_layer,\n",
    "            normalized_shape,\n",
    "            eps=1e-12\n",
    "    ):\n",
    "        super(ModifiedLayerNorm, self).__init__()\n",
    "        # TODO: Do not set bias to 0.\n",
    "        # norm_layer.bias = torch.nn.Parameter(torch.zeros(norm_layer.bias.shape))\n",
    "        \n",
    "        self.norm_layer = norm_layer\n",
    "        self.weight = norm_layer.weight\n",
    "        self.bias = norm_layer.bias\n",
    "        self.normalized_shape = normalized_shape\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            input: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "\n",
    "        z = self.norm_layer(input)\n",
    "        mean = input.mean(dim=-1, keepdim=True)\n",
    "        var = torch.var(input, unbiased=False, dim=-1, keepdim=True)\n",
    "        denominator = torch.sqrt(var + self.eps)\n",
    "        denominator = denominator.detach()\n",
    "        zp = ((input - mean) / denominator) * self.weight + self.bias\n",
    "        zp = stabilize(zp)\n",
    "        return (zp.double() * (z.double() / zp.double()).data.double()).float()\n",
    "    \n",
    "class ModifiedAct(Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        act\n",
    "    ):\n",
    "        super(ModifiedAct, self).__init__()\n",
    "        self.modified_act = nn.Identity()\n",
    "        self.act = act\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        x\n",
    "    ):\n",
    "        z = self.act(x)\n",
    "        zp = self.modified_act(x)\n",
    "        zp = stabilize(zp)\n",
    "        return (zp.double() * (z.double() / zp.double()).data.double()).float()\n",
    "    \n",
    "class ModifiedTanh(Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        act\n",
    "    ):\n",
    "        super(ModifiedTanh, self).__init__() \n",
    "        self.act = act\n",
    "        self.modified_act = nn.Identity()\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        x\n",
    "    ):\n",
    "        z = self.act(x)\n",
    "        zp = self.modified_act(x)\n",
    "        zp = stabilize(zp)\n",
    "        return (zp.double() * (z.double() / zp.double()).data.double()).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e13d92",
   "metadata": {},
   "source": [
    "## LRP Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edd4a749",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gamma(\n",
    "        gam: float = 0.0\n",
    ") -> Tensor:\n",
    "\n",
    "    def modify_parameters(parameters: Tensor):\n",
    "        return parameters + (gam * parameters.clamp(min=0))\n",
    "\n",
    "    return modify_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f3fffb",
   "metadata": {},
   "source": [
    "## BERT Model's Building Blocks\n",
    "\n",
    "<img src=\"BERT.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834bd0b9",
   "metadata": {},
   "source": [
    "## LRP for BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "259fa413",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedBertSelfAttention(nn.Module):\n",
    "    def __init__(self, self_attention):\n",
    "        super(ModifiedBertSelfAttention, self).__init__()\n",
    "        self.query = ModifiedLinear(fc=self_attention.query, transform=gamma())\n",
    "        self.key = ModifiedLinear(fc=self_attention.key, transform=gamma())\n",
    "        self.value = ModifiedLinear(fc=self_attention.value, transform=gamma())\n",
    "        \n",
    "        self.dropout = self_attention.dropout   \n",
    "        self.num_attention_heads = self_attention.num_attention_heads\n",
    "        self.attention_head_size = self_attention.attention_head_size\n",
    "        self.all_head_size = self_attention.all_head_size\n",
    "        \n",
    "    def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "    \n",
    "    def forward(self, hidden_states):\n",
    "        key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
    "        query_layer = self.transpose_for_scores(self.query(hidden_states))\n",
    "        value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
    "        \n",
    "        # Attention scores\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "        \n",
    "        attention_probs = nn.Softmax(dim=-1)(attention_scores).detach()\n",
    "        \n",
    "        \n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        \n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        context_layer = context_layer.view(new_context_layer_shape)\n",
    "        outputs = context_layer\n",
    "        \n",
    "        return outputs\n",
    "        \n",
    "class ModifiedBertSelfOutput(nn.Module): \n",
    "    def __init__(self, self_output):\n",
    "        super(ModifiedBertSelfOutput, self).__init__()\n",
    "        self.dense = ModifiedLinear(fc=self_output.dense, transform=gamma())\n",
    "        self.LayerNorm = ModifiedLayerNorm(norm_layer=self_output.LayerNorm,\n",
    "                                           normalized_shape=self_output.dense.weight.shape[1])\n",
    "        self.dropout = self_output.dropout\n",
    "        \n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        hidden_states = self.dense(hidden_states)                \n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "\n",
    "        return hidden_states\n",
    "        \n",
    "    \n",
    "class ModifiedBertAttention(nn.Module):\n",
    "    def __init__(self, attention):\n",
    "        super(ModifiedBertAttention, self).__init__()\n",
    "        self.self = ModifiedBertSelfAttention(attention.self)\n",
    "        self.output = ModifiedBertSelfOutput(attention.output)\n",
    "        \n",
    "    def forward(self, hidden_states):\n",
    "        self_output = self.self(hidden_states)\n",
    "        attention_output = self.output(self_output, hidden_states)\n",
    "        \n",
    "        return attention_output\n",
    "        \n",
    "        \n",
    "class ModifiedBertIntermediate(nn.Module):\n",
    "    def __init__(self, intermediate):\n",
    "        super(ModifiedBertIntermediate, self).__init__()\n",
    "        self.dense = ModifiedLinear(fc=intermediate.dense, transform=gamma())\n",
    "        self.intermediate_act_fn = ModifiedAct(intermediate.intermediate_act_fn)\n",
    "        \n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.intermediate_act_fn(hidden_states)\n",
    "        return hidden_states  \n",
    "\n",
    "class ModifiedBertOutput(nn.Module):\n",
    "    def __init__(self, output):\n",
    "        super(ModifiedBertOutput, self).__init__()\n",
    "        self.dense = ModifiedLinear(fc=output.dense, transform=gamma())\n",
    "        self.LayerNorm = ModifiedLayerNorm(norm_layer=output.LayerNorm,\n",
    "                                           normalized_shape=output.dense.weight.shape[1])\n",
    "        self.dropout = output.dropout\n",
    "        \n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        hidden_states = self.dense(hidden_states)                \n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor) \n",
    "\n",
    "        return hidden_states\n",
    "    \n",
    "class ModifiedBertLayer(nn.Module):\n",
    "    def __init__(self, layer):\n",
    "        super(ModifiedBertLayer, self).__init__()\n",
    "        self.attention = ModifiedBertAttention(layer.attention)\n",
    "        # self.intermediate = ModifiedBertIntermediate(layer.intermediate)\n",
    "        # self.output = ModifiedBertOutput(layer.output)\n",
    "        self.intermediate = torch.nn.Identity()\n",
    "        self.output = torch.nn.Identity()\n",
    "        \n",
    "    def forward(self, hidden_states):\n",
    "        attention_output = self.attention(hidden_states)\n",
    "        intermediate_output = self.intermediate(attention_output)\n",
    "        hidden_states = self.output(intermediate_output, attention_output)\n",
    "        \n",
    "        return hidden_states\n",
    "\n",
    "# -------- Second-Order --------\n",
    "class ModifiedBertLayerHigherOrder(nn.Module):\n",
    "    def __init__(self, layer):\n",
    "        super(ModifiedBertLayerHigherOrder, self).__init__()\n",
    "        self.attention = ModifiedBertAttention(layer.attention)\n",
    "        \n",
    "    def forward(self, hidden_states, mask):\n",
    "        z = attention_output = self.attention(hidden_states)\n",
    "        z = z * mask + z.data * (1 - mask)\n",
    "        return z\n",
    "# -----------------------------\n",
    "        \n",
    "class ModifiedBertEncoder(nn.Module):\n",
    "    def __init__(self, encoder, order):\n",
    "        super(ModifiedBertEncoder, self).__init__()\n",
    "        self.order = order\n",
    "        layers = []\n",
    "        \n",
    "        n = len(encoder.layer)\n",
    "        for i, layer in enumerate(encoder.layer):\n",
    "            if order == 'higher':\n",
    "                # Mask all layers.\n",
    "                layers.append(ModifiedBertLayerHigherOrder(layer))\n",
    "            else:\n",
    "                layers.append(ModifiedBertLayer(layer))\n",
    "        self.layer = nn.ModuleList(layers)\n",
    "        \n",
    "    def forward(self, hidden_states):        \n",
    "        for i, layer in enumerate(self.layer):\n",
    "            hidden_states = layer(hidden_states)      \n",
    "        return hidden_states\n",
    "    \n",
    "class ModifiedBertPooler(nn.Module):\n",
    "    def __init__(self, pooler):\n",
    "        super(ModifiedBertPooler, self).__init__()\n",
    "        self.dense = ModifiedLinear(fc=pooler.dense, transform=gamma())\n",
    "        self.activation = ModifiedTanh(pooler.activation)\n",
    "        \n",
    "    def forward(self, hidden_states):\n",
    "        first_token_tensor = hidden_states[:, 0]\n",
    "        pooled_output = self.dense(first_token_tensor)\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "        \n",
    "        return pooled_output\n",
    "\n",
    "class ModifiedBertModel(nn.Module):\n",
    "    def __init__(self, bert, embeddings, order):\n",
    "        super(ModifiedBertModel, self).__init__()\n",
    "        self.embeddings = embeddings\n",
    "        self.encoder = ModifiedBertEncoder(bert.encoder, order)\n",
    "        self.pooler = ModifiedBertPooler(bert.pooler)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        hidden_states = self.embeddings(x)    \n",
    "        hidden_states = self.encoder(hidden_states)\n",
    "        hidden_states = self.pooler(hidden_states)\n",
    "        \n",
    "        return hidden_states\n",
    "    \n",
    "class ModifiedBertForSequenceClassification(nn.Module):\n",
    "    def __init__(self, bert_classification, embeddings, order='higher'):\n",
    "        super(ModifiedBertForSequenceClassification, self).__init__()\n",
    "        self.bert = ModifiedBertModel(bert_classification.bert, embeddings, order)\n",
    "        self.dropout = bert_classification.dropout\n",
    "        self.classifier = ModifiedLinear(fc=bert_classification.classifier, transform=gamma())\n",
    "        self.order = order\n",
    "        \n",
    "    def forward(self, x):\n",
    "        hidden_states = self.bert(x)\n",
    "        hidden_states = self.classifier(hidden_states)\n",
    "        \n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edaf0c9",
   "metadata": {},
   "source": [
    "## XAI Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "653cf2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def symb_xai(rels, feats, mode='subset'):\n",
    "    assert mode in ['subset', 'and', 'or', 'not'], f'Mode \"{mode}\" is not implemented.'\n",
    "    r = 0.\n",
    "    for w, rel in rels.items():\n",
    "        if mode == 'subset' and all([token in feats for token in w ]):\n",
    "            r += rel\n",
    "        elif mode == 'and' and all([ token in w for token in feats ]):\n",
    "            r += rel\n",
    "        elif mode == 'or' and any([ token in w for token in feats]):\n",
    "            r += rel\n",
    "        elif mode == 'not' and all([ token not in w for token in feats]):\n",
    "            r += rel\n",
    "    return r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3fd8de",
   "metadata": {},
   "source": [
    "## LRP for BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b9a74c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lrp(model, x, target, indices, pretrained_embeddings):\n",
    "    A = {}\n",
    "    \n",
    "    hidden_states = pretrained_embeddings(input_ids=x['input_ids'], token_type_ids=x['token_type_ids'])\n",
    "    A['hidden_states'] = hidden_states\n",
    "    attn_input = hidden_states\n",
    "    \n",
    "    if model.order == 'higher':\n",
    "        M = torch.eye(hidden_states.shape[1])\n",
    "        j, k, l = indices\n",
    "        Mj = M[j].unsqueeze(0).unsqueeze(2)\n",
    "        Mk = M[k].unsqueeze(0).unsqueeze(2)\n",
    "        Ml = M[l].unsqueeze(0).unsqueeze(2)\n",
    "        \n",
    "    n = len(model.bert.encoder.layer)\n",
    "    for i, layer in enumerate(model.bert.encoder.layer):\n",
    "            attn_inputdata = attn_input.data\n",
    "            attn_inputdata.requires_grad_(True) \n",
    "                        \n",
    "            A['attn_input_{}_data'.format(i)] = attn_inputdata\n",
    "            A['attn_input_{}'.format(i)] = attn_input\n",
    "            \n",
    "            if model.order == 'higher':\n",
    "                if i == n-1:\n",
    "                    output = model.bert.encoder.layer[i](A['attn_input_{}_data'.format(i)], Mj)\n",
    "                elif i == n-2:\n",
    "                    output = model.bert.encoder.layer[i](A['attn_input_{}_data'.format(i)], Mk)\n",
    "                elif i == n-3:\n",
    "                    output = model.bert.encoder.layer[i](A['attn_input_{}_data'.format(i)], Ml)  \n",
    "            else:\n",
    "                output = model.bert.encoder.layer[i](A['attn_input_{}_data'.format(i)])\n",
    "            attn_input = output\n",
    "            \n",
    "    outputdata = output.data\n",
    "    outputdata.requires_grad_(True)\n",
    "    \n",
    "    pooled = model.bert.pooler(outputdata)\n",
    "    pooleddata = pooled.data\n",
    "    pooleddata.requires_grad_(True) \n",
    "        \n",
    "    logits = model.classifier(pooleddata)\n",
    "    pred = torch.argmax(logits)\n",
    "    Rout = (logits * target).sum()\n",
    "    \n",
    "    Rout.backward()\n",
    "    ((pooleddata.grad)*pooled).sum().backward()\n",
    "                \n",
    "    Rpool = ((outputdata.grad)*output)\n",
    "    R_ = Rpool\n",
    "    \n",
    "    R_all = []\n",
    "    for i, layer in list(enumerate(model.bert.encoder.layer))[::-1]:\n",
    "        R_.sum().backward()\n",
    "        R_grad = A['attn_input_{}_data'.format(i)].grad\n",
    "        R_attn =  (R_grad)*A['attn_input_{}'.format(i)]\n",
    "        \n",
    "        R_ = R_attn\n",
    "        R_all.append(R_.sum(2).detach().cpu().numpy().squeeze().sum())\n",
    "\n",
    "    R = R_.sum(2).detach().cpu().numpy()\n",
    "    \n",
    "    return R, pred, Rout, R_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677164c4",
   "metadata": {},
   "source": [
    "## BERT model with 3 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84a88209",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = torch.load('sst2-3layer-model.pt', map_location=torch.device('cpu'))\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"textattack/bert-base-uncased-SST-2\")\n",
    "model.bert.encoder.layer = model.bert.encoder.layer[:3]\n",
    "\n",
    "model.bert.encoder.layer[0].intermediate = torch.nn.Identity()\n",
    "model.bert.encoder.layer[1].intermediate = torch.nn.Identity()\n",
    "model.bert.encoder.layer[2].intermediate = torch.nn.Identity()\n",
    "\n",
    "model.bert.encoder.layer[0].output = torch.nn.Identity()\n",
    "model.bert.encoder.layer[1].output = torch.nn.Identity()\n",
    "model.bert.encoder.layer[2].output = torch.nn.Identity()\n",
    "\n",
    "state_dict = model.state_dict()   \n",
    "\n",
    "for i in range(3):\n",
    "    state_dict['bert.encoder.layer.{}.attention.self.query.weight'.format(i)] = params['attention_layers.{}.query.weight'.format(i)]\n",
    "    state_dict['bert.encoder.layer.{}.attention.self.key.weight'.format(i)] = params['attention_layers.{}.key.weight'.format(i)]\n",
    "    state_dict['bert.encoder.layer.{}.attention.self.value.weight'.format(i)] = params['attention_layers.{}.value.weight'.format(i)]\n",
    "    \n",
    "    state_dict['bert.encoder.layer.{}.attention.self.query.bias'.format(i)] = params['attention_layers.{}.query.bias'.format(i)]\n",
    "    state_dict['bert.encoder.layer.{}.attention.self.key.bias'.format(i)] = params['attention_layers.{}.key.bias'.format(i)]\n",
    "    state_dict['bert.encoder.layer.{}.attention.self.value.bias'.format(i)] = params['attention_layers.{}.value.bias'.format(i)]\n",
    "    \n",
    "    state_dict['bert.encoder.layer.{}.attention.output.dense.weight'.format(i)] = params['attention_layers.{}.output.dense.weight'.format(i)]\n",
    "    state_dict['bert.encoder.layer.{}.attention.output.dense.bias'.format(i)] = params['attention_layers.{}.output.dense.bias'.format(i)]\n",
    "    \n",
    "    state_dict['bert.encoder.layer.{}.attention.output.LayerNorm.weight'.format(i)] = params['attention_layers.{}.output.LayerNorm.weight'.format(i)]\n",
    "    state_dict['bert.encoder.layer.{}.attention.output.LayerNorm.bias'.format(i)] = params['attention_layers.{}.output.LayerNorm.bias'.format(i)]\n",
    "    \n",
    "    state_dict['bert.pooler.dense.weight'] = params['pooler.dense.weight']\n",
    "    state_dict['bert.pooler.dense.bias'] = params['pooler.dense.bias']\n",
    "    \n",
    "    state_dict['classifier.weight'] = params['classifier.weight']\n",
    "    state_dict['classifier.bias'] = params['classifier.bias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca4c78bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2e54c1",
   "metadata": {},
   "source": [
    "## Explain tiny BERT model on the SST2 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "627b4cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset sst2 (/home/farnoush/.cache/huggingface/datasets/sst2/default/2.0.0/9896208a8d85db057ac50c72282bcb8fe755accc671a57dd8059d4e130961ed5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e07aeae2fc3a4a8b9158551fbe0083dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_data = 0\n",
    "relevance = []\n",
    "relevance_all = []\n",
    "logits = []\n",
    "acc = 0\n",
    "\n",
    "# Load SST2 dataset.\n",
    "dataset = load_dataset(\"sst2\", \"default\")\n",
    "\n",
    "# Load model.\n",
    "# model = BertForSequenceClassification.from_pretrained(\"textattack/bert-base-uncased-SST-2\")\n",
    "pretrained_embeddings = model.bert.embeddings\n",
    "modified_model = ModifiedBertForSequenceClassification(model,\n",
    "                                                       pretrained_embeddings,\n",
    "                                                       order='higher')\n",
    "modified_model.eval()\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"textattack/bert-base-uncased-SST-2\")\n",
    "UNK_IDX = tokenizer.unk_token_id  # an out-of-vocab token\n",
    "\n",
    "i = 1\n",
    "sentence = dataset['validation']['sentence'][i]\n",
    "target = torch.nn.functional.one_hot(\n",
    "    torch.tensor(dataset['validation']['label'][i]), num_classes=2)\n",
    "\n",
    "x = tokenizer(sentence, return_tensors=\"pt\")\n",
    "words = tokenizer.convert_ids_to_tokens(x['input_ids'].squeeze())\n",
    "\n",
    "pairs = {}\n",
    "set(product(range(len(words)),repeat = 4))\n",
    "combinations = set(product(range(len(words)),repeat = 4))\n",
    "for (i, j, k, l) in combinations:\n",
    "    indices = (j, k, l)\n",
    "    R, pred, logit, R_all = lrp(modified_model, x, target, indices, pretrained_embeddings)\n",
    "    pairs[(i, j, k, l)] = R.squeeze()[i].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b7d4da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------\n",
      "Effect of feature 'bleak':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"background-color:#fffefe\">[CLS]</span> <span style=\"background-color:#fffefe\">un</span> <span style=\"background-color:#fffefe\">##fl</span> <span style=\"background-color:#fffefe\">##in</span> <span style=\"background-color:#fffefe\">##ching</span> <span style=\"background-color:#fffefe\">##ly</span> <span style=\"background-color:#ff0000\">bleak</span> <span style=\"background-color:#fffefe\">and</span> <span style=\"background-color:#fffefe\">desperate</span> <span style=\"background-color:#fffefe\">[SEP]</span> \n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------\n",
      "Effect of feature 'desperate':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"background-color:#fffefe\">[CLS]</span> <span style=\"background-color:#fffefe\">un</span> <span style=\"background-color:#fffefe\">##fl</span> <span style=\"background-color:#fffefe\">##in</span> <span style=\"background-color:#fffefe\">##ching</span> <span style=\"background-color:#fffefe\">##ly</span> <span style=\"background-color:#fffefe\">bleak</span> <span style=\"background-color:#fffefe\">and</span> <span style=\"background-color:#ff0000\">desperate</span> <span style=\"background-color:#fffefe\">[SEP]</span> \n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------\n",
      "Joint effect of features 'bleak' and 'desperate':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"background-color:#fffefe\">[CLS]</span> <span style=\"background-color:#fffefe\">un</span> <span style=\"background-color:#fffefe\">##fl</span> <span style=\"background-color:#fffefe\">##in</span> <span style=\"background-color:#fffefe\">##ching</span> <span style=\"background-color:#fffefe\">##ly</span> <span style=\"background-color:#ff0000\">bleak</span> <span style=\"background-color:#fffefe\">and</span> <span style=\"background-color:#ff0000\">desperate</span> <span style=\"background-color:#fffefe\">[SEP]</span> \n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------\n",
      "------------------\n",
      "Joint effect of features 'bleak', 'and', and 'desperate':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"background-color:#fffefe\">[CLS]</span> <span style=\"background-color:#fffefe\">un</span> <span style=\"background-color:#fffefe\">##fl</span> <span style=\"background-color:#fffefe\">##in</span> <span style=\"background-color:#fffefe\">##ching</span> <span style=\"background-color:#fffefe\">##ly</span> <span style=\"background-color:#ff0000\">bleak</span> <span style=\"background-color:#ff0000\">and</span> <span style=\"background-color:#ff0000\">desperate</span> <span style=\"background-color:#fffefe\">[SEP]</span> \n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"------------------\")\n",
    "print(\"Effect of feature 'bleak':\")\n",
    "R = np.zeros(len(words))\n",
    "R_6 = symb_xai(pairs, (6,), mode='and')\n",
    "R[6] = R_6\n",
    "display(HTML(html_heatmap(words, R.squeeze())))\n",
    "\n",
    "print(\"------------------\")\n",
    "print(\"Effect of feature 'desperate':\")\n",
    "R = np.zeros(len(words))\n",
    "R_8 = symb_xai(pairs, (8,), mode='and')\n",
    "R[8] = R_8\n",
    "display(HTML(html_heatmap(words, R.squeeze())))\n",
    "\n",
    "print(\"------------------\")\n",
    "print(\"Joint effect of features 'bleak' and 'desperate':\")\n",
    "R = np.zeros(len(words))\n",
    "R_68 = symb_xai(pairs, (6, 8), mode='and')\n",
    "R[6] = R_68\n",
    "R[8] = R_68\n",
    "display(HTML(html_heatmap(words, R.squeeze())))\n",
    "print(\"------------------\")\n",
    "\n",
    "print(\"------------------\")\n",
    "print(\"Joint effect of features 'bleak', 'and', and 'desperate':\")\n",
    "R = np.zeros(len(words))\n",
    "R_678 = symb_xai(pairs, (6, 7, 8), mode='and')\n",
    "R[6] = R_678\n",
    "R[7] = R_678\n",
    "R[8] = R_678\n",
    "display(HTML(html_heatmap(words, R.squeeze())))\n",
    "print(\"------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d49dcd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
